---
title: "Lab 10b - Boosting"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = T, include  = T)
```

# Learning goals

- Perform gradient boosting and extreme gradient boosting on the ``heart`` data.
- Compare the performance of the two.

# Lab description

For this lab we will be working with the `heart` dataset that you can download from [here](https://github.com/JSC370/jsc370-2022/blob/main/data/heart/heart.csv)


### Setup packages

You should install and load `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r, eval = FALSE, message = FALSE, warning=FALSE}
install.packages(c("gbm","xgboost","caret"))
```

### Load packages and data
```{r, warning=FALSE, message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(gbm)
library(xgboost)
library(caret)

heart<-read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/heart/heart.csv")
```


---

## Question 1: Gradient Boosting

Evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction).  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

Note, boosting can overfit if the number of trees is too large. The shrinkage parameter controls the rate at which the boosting learns. Very small $\lambda$ can require using a very large number of trees to achieve good performance. Finally, interaction depth controls the interaction order of the boosted model. A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. the default is 1.

i. Split the heart data into training and testing. Also need to make character variables into numeric variables and get rid of missing values.

```{r}
set.seed(301)

heart$AHD_num <- ifelse(heart$AHD=="Yes",1,0)
heart$ChestPain_num <- ifelse(heart$ChestPain=="asymptomatic",1,ifelse(heart$ChestPain=="nonanginal",2,ifelse(heart$ChestPain=="nontypical",3,0)))
heart$Thal_num <- ifelse(heart$Thal=="fixed",1,ifelse(heart$Thal=="normal",2,0))
heart <- heart %>% select(-c(AHD, ChestPain, Thal))
heart <-na.omit(heart)

train = sample(1:nrow(heart), floor(nrow(heart) * 0.7))
test = setdiff(1:nrow(heart), train)
```


ii. Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration and calculate the test MSE.

```{r}
set.seed(301)
heart_boost = gbm(AHD_num ~., data = heart[train,], distribution = 'bernoulli',
                 n.trees = 5000, shrinkage = 0.001, interaction.depth = 1,
                 cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost)

plot(heart_boost$train.error)
lines(heart_boost$cv.error, col = 'blue')

yhat_boost <- predict(heart_boost, newdata = heart[test,], n.trees = 5000)
mean((yhat_boost - heart[test, "AHD_num"])^2)
```

iii. Repeat ii. using the same seed and ``n.trees=5000`` with the following 3 additional combination of parameters: a) ``shrinkage = 0.001``, ``interaction.depth = 2``; b) ``shrinkage = 0.01``, ``interaction.depth = 1``; c) ``shrinkage = 0.01``, ``interaction.depth = 2``.

```{r}
set.seed(301)
heart_boost2 = gbm(AHD_num ~., data = heart[train,], distribution = 'bernoulli',
                   n.trees = 5000, shrinkage = 0.001, interaction.depth = 2,
                   cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost2)

plot(heart_boost2$train.error)
lines(heart_boost2$cv.error, col = 'blue')

yhat_boost2 <- predict(heart_boost2, newdata = heart[test,], n.trees = 5000)
mean((yhat_boost2 - heart[test, "AHD_num"])^2)
```

```{r}
set.seed(301)
heart_boost3 = gbm(AHD_num ~., data = heart[train,], distribution = 'bernoulli',
                   n.trees = 5000, shrinkage = 0.01, interaction.depth = 1,
                   cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost3)

plot(heart_boost3$train.error)
lines(heart_boost3$cv.error, col = 'blue')

yhat_boost3 <- predict(heart_boost3, newdata = heart[test,], n.trees = 5000)
mean((yhat_boost3 - heart[test, "AHD_num"])^2)
```

```{r}
set.seed(301)
heart_boost4 = gbm(AHD_num ~., data = heart[train,], distribution = 'bernoulli',
                   n.trees = 5000, shrinkage = 0.01, interaction.depth = 2,
                   cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost4)

plot(heart_boost4$train.error)
lines(heart_boost4$cv.error, col = 'blue')

yhat_boost4 <- predict(heart_boost4, newdata = heart[test,], n.trees = 5000)
mean((yhat_boost4 - heart[test, "AHD_num"])^2)
```


## Question 2: Extreme Gradient Boosting
Training an xgboost model with `xgboost` and perform a grid search for tuning the number of trees and the maxium depth of the tree. Also perform 10-fold cross-validation and determine the variable importance. Finally, compute the test MSE.

```{r, warning = FALSE}
train_control = trainControl(method = "cv", number = 10, search = "grid")
tune_grid <- expand.grid(max_depth = c(1, 3, 5, 7),
                         nrounds = c(1:10) * 50,
                         eta = c(0.01, 0.1, 0.3),
                         gamma = 0,
                         subsample = 1,
                         min_child_weight = 1,
                         colsample_bytree = 0.6)

heart_xgb <- caret::train(AHD_num ~ ., data = heart[train,], method = "xgbTree",
                          trControl = train_control, tuneGrid = tune_grid)
                         
plot(varImp(heart_xgb, scale = F))

yhat_xgb <- predict(heart_xgb, newdata = heart[test,])
mean((yhat_xgb - heart[test, "AHD_num"])^2)
caret::RMSE(heart[test, "AHD_num"], yhat_xgb)^2
```


